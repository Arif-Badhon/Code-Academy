Just when you thought SDG solved all our problems, even more options come into the picture!

There are also other variants of gradient descent such as Adam optimization algorithm and mini-batch gradient descent. 
Adam is an adaptive learning algorithm that finds individual learning rates for each parameter. 
Mini-batch gradient descent is similar to SGD except instead of iterating on one data point at a time, we iterate on small batches of fixed size.

Adam optimizerâ€™s ability to have an adaptive learning rate has made it an ideal variant of gradient descent and is commonly 
used in deep learning models. Mini-batch gradient descent was developed as an ideal trade-off between GD and SGD. 
Since mini-batch does not depend on just one training sample, it has a much smoother curve and is less affected by 
outliers and noisy data making it a more optimal algorithm for gradient descent than SGD.

These are just some quick notes! You can read more about Adam here and more about mini-batch here. 
Experts in deep learning are constantly coming up with ways to improve these algorithms to make them more efficient and accurate, 
so the ability to adapt and build upon what you learn as you dive into this domain will be key!
